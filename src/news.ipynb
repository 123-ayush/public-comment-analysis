{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5bec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5725d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“° Scraping News Article Comments...\n",
      "\n",
      "ðŸ” Parsing: https://www.ndtv.com/india-news/example-article-url\n",
      "âš  No comments found using basic selectors\n",
      "\n",
      "ðŸ” Parsing: https://indianexpress.com/article/example/\n",
      "âš  No comments found using basic selectors\n",
      "\n",
      "ðŸ” Parsing: https://theprint.in/example-policy-article/\n",
      "âš  No comments found using basic selectors\n",
      "\n",
      "ðŸŽ‰ Scraping completed!\n",
      "ðŸ’¾ Saved at: C:\\Users\\Ayush Ahlawat\\OneDrive\\Documents\\Public Comment Analysis\\public-comment-analysis\\dataset\\news_comments_raw.csv\n",
      "ðŸ—‚ Total comments collected: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“° Scraping News Article Comments...\")\n",
    "\n",
    "# Add article URLs here\n",
    "urls = [\n",
    "    \"https://www.ndtv.com/india-news/example-article-url\",\n",
    "    \"https://indianexpress.com/article/example/\",\n",
    "    \"https://theprint.in/example-policy-article/\"\n",
    "]\n",
    "\n",
    "comments_data = []\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"\\nðŸ” Parsing: {url}\")\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Common comment selectors (will detect any of these)\n",
    "    selectors = [\n",
    "        \".comment-body\", \".comment-content\", \".comment-text\", \n",
    "        \".c-message\", \".livefyre-comment\", \".fb-comments\"\n",
    "    ]\n",
    "\n",
    "    found_comments = False\n",
    "    for sel in selectors:\n",
    "        comments = soup.select(sel)\n",
    "        if comments:\n",
    "            found_comments = True\n",
    "            for c in comments:\n",
    "                comment_text = c.get_text(\" \", strip=True)\n",
    "                if len(comment_text) > 40:\n",
    "                    comments_data.append({\"source_url\": url, \"comment_text\": comment_text})\n",
    "\n",
    "    if not found_comments:\n",
    "        print(\"âš  No comments found using basic selectors\")\n",
    "\n",
    "# Save file\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "output_path = r\"C:\\Users\\Ayush Ahlawat\\OneDrive\\Documents\\Public Comment Analysis\\public-comment-analysis\\dataset\\news_comments_raw.csv\"\n",
    "pd.DataFrame(comments_data).to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Scraping completed!\")\n",
    "print(f\"ðŸ’¾ Saved at: {output_path}\")\n",
    "print(f\"ðŸ—‚ Total comments collected: {len(comments_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
